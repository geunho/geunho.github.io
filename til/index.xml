<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Tils on Blog · 김근호(Kim, Geunho)</title>
    <link>https://blog.geunho.dev/til/</link>
    <description>Recent content in Tils on Blog · 김근호(Kim, Geunho)</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 16 Jun 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://blog.geunho.dev/til/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Hive - implicit join 특성</title>
      <link>https://blog.geunho.dev/til/hive/implicit-join/</link>
      <pubDate>Tue, 16 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://blog.geunho.dev/til/hive/implicit-join/</guid>
      <description>implicit join은 SQL 구문에서 join절을 직접 쓰지 않고 문자 그대로 암시적 구문으로써 join 하는 것을 뜻한다.
INNER JOIN
-- INNER JOIN SELECT a.id, b.name FROM a INNER JOIN b ON a.id = b.id -- IMPLICIT INNER JOIN SELECT a.id, b.name FROM a, b WHERE a.id = b.id implicit join은 두 번째 쿼리와 같이 대상 테이블을 FROM절 이후 콤마,로 구분해서 나열하며, WHERE절에 join 조건을 달게 된다.
두 쿼리가 매우 유사하지만, implicit join에서 이 WHERE절 조건에 따라 어떤 join이 될지 달라지게 된다.</description>
    </item>
    
    <item>
      <title>GitHub - 코드 검색시 주의사항</title>
      <link>https://blog.geunho.dev/til/github/cdoe-search/</link>
      <pubDate>Mon, 15 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://blog.geunho.dev/til/github/cdoe-search/</guid>
      <description>GitHub은 repository의 default 브랜치의 코드만 색인한다. 만약 버전 혹은 컴포넌트에 따라 서로 다른 브랜치로 관리하는 전략을 사용한다면, 추후 관련 코드가 색인되지 않아 코드 검색 기반 의사 결정시 잘 못된 결정을 내릴 수 있다.
 Only the default branch is indexed for code search. In most cases, this will be the master branch.
 참고 https://help.github.com/en/github/searching-for-information-on-github/searching-code</description>
    </item>
    
    <item>
      <title>MapReduce - Reduce-side join</title>
      <link>https://blog.geunho.dev/til/hadoop/reduce-side-join/</link>
      <pubDate>Mon, 15 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://blog.geunho.dev/til/hadoop/reduce-side-join/</guid>
      <description>SQL에서는 단일 구문 하나로 여러 테이블을 쉽게 join 할 수 있다. 구문 분석기가 데이터를 다루는 복잡한 부분은 알아서 처리해주는 덕분인데, MapReduce(이하 MR)로 join을 구현하려면 이러한 &amp;ldquo;알아서 처리해주는&amp;rdquo; 일을 직접 코드로 다뤄야 한다.
MR join 중 가장 간단히 모든 join을 구현할 수 있는 방법이지만 비용이 비싼 Reduce-side join을 정리한다.
Reduce-side join reduce 단계에서 데이터 셋을 join하는 처리 방식을 reduce-side join이라 한다.
데이터 크기와 상관없이 inner join, left outer join, right outer join, full outer join, exclusive join, 그리고 Cartesian product까지 모든 종류의 join을 구현할 수 있다.</description>
    </item>
    
    <item>
      <title>Maven - Java cannot find symbol: javax.annotation.Resource</title>
      <link>https://blog.geunho.dev/til/maven/java-cannot-find-symbol-resource/</link>
      <pubDate>Mon, 15 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://blog.geunho.dev/til/maven/java-cannot-find-symbol-resource/</guid>
      <description>다음과 같은 빌드 오류가 발생했을 때 해결 방법을 정리한다.
Java cannot find symbol: javax.annotation.Resource javax.annotation.Resource 클래스를 찾을 수 없다는 오류인데, 이는 annotation으로 사용된다.
@Resource Foo bar 해결 방안 JDK 11 빌드 환경에서는 Resource 클래스를 찾을 수 있도록 추가하거나,
아래와 같이 Resource annotation 대신 Autowired, Qualifier를 사용한다.
@Autowired @Qualifier(&amp;#34;bar&amp;#34;) Foo bar 변경이 부득이한 경우에는, JDK 11 -&amp;gt; JDK 8 로 빌드 버전을 낮추면 해결된다.</description>
    </item>
    
    <item>
      <title>Debezium - CDC를 위한 Connector 정리</title>
      <link>https://blog.geunho.dev/til/debezium/connectors/</link>
      <pubDate>Wed, 10 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://blog.geunho.dev/til/debezium/connectors/</guid>
      <description>Debezium은 변경 데이터 캡처(change data capture, 이하 CDC)를 구현하는 분산 플랫폼이다.
과거에는 데이터베이스의 변경 로그를 내부 구현으로만 가지고 있었기 때문에 사용자는 직접 주기적인 쿼리를 하거나, 비용이 값비싼 트리거와 같은 방식을 이용해야 했다. Debezium을 비롯해 여러 솔루션들은 이러한 방식에서 직접 데이터베이스의 복제 로그를 추적해서 스트림으로 제공하는 방식으로 CDC를 구현하고 있다.
Debezium은 이러한 스트림을 제공하기 위해, Kafka 플랫폼의 엔터프라이즈 솔루션을 제공하는 Confluent사에서 개발한 Kafka Connect의 Connector 구현체로 CDC를 제공한다.
Debezium Connector는 Kafka Connect 기반으로 동작한다.</description>
    </item>
    
    <item>
      <title>Database - index에 대해서</title>
      <link>https://blog.geunho.dev/til/database/</link>
      <pubDate>Tue, 09 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://blog.geunho.dev/til/database/</guid>
      <description>index는 데이터베이스(이하 DB)에 순차적으로 저장된 레코드를 빠르게 찾기 위한 자료구조다. DB 내부 엔진마다 구현된 내용은 다르겠지만, 일반적인 관계형 DB의 index는 B+트리 구조로 되어있다.
레코드와 index가 물리적으로 저장되는 방식에 따라 index는 Clustered index, Non-clusterd index 두 종류로 나뉜다.
Clustered index Clustered index는 하나의 테이블이 하나만 가질 수 있는 index인데, 이 index의 순서에 따라 실제로 레코드가 함께 저장된다. 테이블의 PK index가 이에 해당된다.
Non-clustered index index가 레코드가 저장되는 공간과 별도의 공간에 저장되며 테이블은 여러 개의 non-clustered index를 가질 수 있다.</description>
    </item>
    
    <item>
      <title>Git - remote에 커밋한 내용 되돌리기</title>
      <link>https://blog.geunho.dev/til/git/rollback/</link>
      <pubDate>Mon, 08 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://blog.geunho.dev/til/git/rollback/</guid>
      <description>git에서 원격 저장소에 이미 커밋한 내용을 되돌리는(rollback) 방법은 두 가지가 있다:
 특정 커밋으로 강제 push revert 커밋 생성 후 push  특정 커밋으로 강제 push 아주 간단하고 강력한 방법으로, 원격 저장소에 커밋한 내용을 완전히 삭제할 수 있다. 개인용이거나 작업자가 많지 않은 저장소, 혹은 아직 병합(merge)되지 않은 브랜치와 같은 곳에서 효과적으로 사용할 수 있다.
명령어도 간단하다. 다음과 같은 한 줄이면 즉시 원격 저장소의 커밋을 손쉽게 되돌린다.
git push -f origin $hash:$remote_branch $remote_branch 원격 브랜치를 $hash 커밋으로 -f 강제 push 하겠다는 내용이다.</description>
    </item>
    
    <item>
      <title>Maven - 의존성 관리시 주의사항</title>
      <link>https://blog.geunho.dev/til/maven/dependencies/</link>
      <pubDate>Mon, 08 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://blog.geunho.dev/til/maven/dependencies/</guid>
      <description>Maven 프로젝트는 외부 모듈을 참조할때 maven repository로부터 참조 대상을 POM 파일의 의존성 항목에 추가해서 관리한다.
&amp;lt;dependencies&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;$groupId&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;$artifactId&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;$version&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; &amp;lt;/dependencies&amp;gt; 참조로 하는 모듈은 maven import시 프로젝트의 classpath에 참조 대상 라이브러리로 추가되고, 만약 그 모듈이 다시 참조하는 모듈이 있는 경우 그 모듈도 classpath에 함께 추가된다.
이로인해 발생하는 주의해야 할 부분이 발생한다.
 classpath에 참조의 하위 참조도 추가되므로 root 프로젝트에서 해당 모듈에 접근할 수 있음 만약 root 프로젝트에서 하위 참조를 즉시 참조로 추가해서 사용할 경우 버전 충돌이 일어날 수 있음 만약 즉시 참조없이 사용할 경우 상위 모듈 참조를 제거했을 때 빌드 오류가 발생할 수 있음  따라서 어떤 모듈을 의존성에 추가할때, 하위 참조 모듈은 classpath에 추가되지 않도록 설정하거나 -(1) 상위 모듈을 제거할 때 하위 참조 모듈을 명시적으로 의존성에 추가 -(2) 해줘야 한다.</description>
    </item>
    
    <item>
      <title>GitHub - 코멘트에 접어두는 코드 삽입하기</title>
      <link>https://blog.geunho.dev/til/github/markdown-tips-1/</link>
      <pubDate>Fri, 05 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://blog.geunho.dev/til/github/markdown-tips-1/</guid>
      <description>GitHub에서 마크다운 형식의 코멘트를 남기다보면 장황한 부가 설명은 접어두는 블럭으로 지정할 수 있다.
접어두는 블럭은 details, summary 태그를 코멘트에 직접 넣어주면 되는데 다음과 같이 사용할 수 있다.
&amp;lt;details&amp;gt; &amp;lt;summary&amp;gt;더 보기...&amp;lt;/summary&amp;gt; 장황한 부연 설명들 &amp;lt;/details&amp;gt; &amp;gt; 더 보기... (클릭해서 펼치고 접어둘 수 있음) 이때 코드 뭉치를 넣을 수 도 있는데, 한 줄 넘기고 작성하면 잘 동작한다.
&amp;lt;details&amp;gt; &amp;lt;summary&amp;gt;더 보기...&amp;lt;/summary&amp;gt; [장황한 code block] &amp;lt;/details&amp;gt; </description>
    </item>
    
    <item>
      <title>Redis - 조회 명령어 정리</title>
      <link>https://blog.geunho.dev/til/redis/ops/</link>
      <pubDate>Thu, 04 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://blog.geunho.dev/til/redis/ops/</guid>
      <description>redis-cli를 이용해서 간단히 조회할 때 사용할만한 redis 명령어 몇 가지를 정리한다.
get string 형식으로 저장된 값 하나를 조회한다.
get my-key &amp;#34;my value blah blah&amp;#34; hget hash로 저장된 값의 필드 하나를 조회한다.
# my-hkey로 저장된 hash의 my-field에 대한 값을 조회 hget my-hkey my-field &amp;#34;my field value blah blah&amp;#34; lrange list로 저장된 목록을 조회한다.
# 전체 목록 조회 lrange my-lkey 0 -1 # 일부 목록 조회, 시작 인덱스와 끝 인덱스를 지정 # 끝 인덱스는 결과에 포함됨 lrange my-lkey 1 4 smembers set으로 저장된 모든 값을 조회한다.</description>
    </item>
    
    <item>
      <title>from_timestamp - Impala 날짜 형식 사용하기</title>
      <link>https://blog.geunho.dev/til/impala/from_timestamp/</link>
      <pubDate>Wed, 03 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://blog.geunho.dev/til/impala/from_timestamp/</guid>
      <description>일반적인 RDB에 내장된 to_char(timestamp, text) 함수가 Impala에는 없다. 대신, 같은 역할을 하면서 이름만 다른 from_timestamp(TIMESTAMP datetime, STRING pattern) 함수를 사용하면 된다.
# ANSI SELECT to_char(now(), &amp;#39;yyyy-MM-dd, HH:mm&amp;#39;) # Impala SELECT from_timestamp(now(), &amp;#39;yyyy-MM-dd, HH:mm&amp;#39;) 참고  Impala Date and Time Functions#FROM_TIMESTAMP, Impala docs 9.8. Data Type Formatting Functions, Postgresql docs  </description>
    </item>
    
    <item>
      <title>Kudu - 기본 내용 정리 (1)</title>
      <link>https://blog.geunho.dev/til/kudu/101/</link>
      <pubDate>Wed, 03 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://blog.geunho.dev/til/kudu/101/</guid>
      <description>101 Apache Kudu는 Cloudera에서 개발한 컬럼 기반(Columnar) 저장소이다. 저장소 자체는 다른 서비스(예. HDSF)에 대한 의존 없이, Kudu 인스턴스(Tablet)만으로 클러스터를 이루며 리더 선정도 내장된 알고리즘(Raft consensus algorithm)에 의해 정해진다.
Kudu는 물리적으로 레코드를 컬럼 단위로 저장하기 때문에, 일부 컬럼에 대한 질의시 네트워크 비용을 아끼면서 효율적으로 레코드를 조회할 수 있다. (컬럼 지향(Column oriented) 저장소인 HBase는 물리적으로 로우 단위로 저장)
Kudu에 질의하기 위해서는 별도의 질의 엔진이 필요하다. MR, Spark와 같은 Hadoop 계열 엔진을 사용할 수 있는데, 아무래도 같은 Cloudera에서 개발한 Impala를 권장한다.</description>
    </item>
    
    <item>
      <title>har - Hadoop archive 도구로 압축하기</title>
      <link>https://blog.geunho.dev/til/hadoop/har/</link>
      <pubDate>Tue, 02 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://blog.geunho.dev/til/hadoop/har/</guid>
      <description>hadoop 명령어에는 HDFS 상 파일들을 압축하기 위한 archive 도구가 있다. 명령어 한 줄로 여러 경로의 입력들을 하나로 압축할 수 있다.
HDFS의 블록 크기보다 작은 다수 개의 파일은 namenode의 메모리 공간을 낭비하는데, 이러한 파일들을 har 압축하면 사용량을 줄일 수 있다.
단, 압축시 원본에 대해 복사본을 생성하는 것이므로 원본 파일 용량만큼의 공간은 확보되어 있어야 한다. 그리고 하나의 파일로 압축하더라도, MR 입력시 har 단일 파일로 인식하지 않고 원본 파일 개수 만큼을 입력으로 가지므로 MR의 작업 효율이 좋아지지는 않는다.</description>
    </item>
    
    <item>
      <title>sh - sed로 마지막 N개 문자열 제거하기</title>
      <link>https://blog.geunho.dev/til/sh/sed-tips-1/</link>
      <pubDate>Sun, 31 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://blog.geunho.dev/til/sh/sed-tips-1/</guid>
      <description>쉘에서 sed (stream editor) 도구를 사용하면 다채로운 문자열 조작을 할 수 있다.
사용방법 아래는 마지막 2개 문자열을 제거하는 예제이다.
# sed s/regexp/replacement/ sed &amp;#39;s/.\{2\}$//&amp;#39; &amp;lt;&amp;lt;&amp;lt; &amp;#34;Hello, Bash&amp;#34; Hello, Ba # 위 예시는 마지막 2개 문자를 공백으로 치환한 것. 따라서 다른 문자열로 치환도 가능하다. sed &amp;#39;s/.\{2\}$/bo/&amp;#39; &amp;lt;&amp;lt;&amp;lt; &amp;#34;Hello, Bash&amp;#34; Hello, Babo 참고 https://linux.die.net/man/1/sed</description>
    </item>
    
    <item>
      <title>distcp - HDFS 클러스터 간 파일 복제하기</title>
      <link>https://blog.geunho.dev/til/hadoop/distcp/</link>
      <pubDate>Fri, 29 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://blog.geunho.dev/til/hadoop/distcp/</guid>
      <description>distcp는 distributed copy의 약어로, 서로다른 HDFS 클러스터 간 파일을 복제할때 사용하는 명령어이다.
사용방법 hadoop distcp hdfs://nn1/job/logs hdfs://nn1/job/others hdfs://nn2/job/ nn2 hdfs에 nn1의 두 디렉토리를 복제한다. 공백으로 구분해서 입력 경로를 여러 개를 지정할 수 있고, 마지막 인자에 출력 경로를 지정하면 된다.
hadoop distcp -D dfs.replication=2 -m 32 hdfs://nn1/job/logs hdfs://nn1/job/others hdfs://nn2/job/ -D 옵션으로 hadoop 프로그램에 일반적인 옵션을 지정할 수 있고, -m 옵셥으로 map 수를 지정하여 최대 동시 복제 수를 설정한다.
참고 DistCp, Hadoop docs</description>
    </item>
    
  </channel>
</rss>