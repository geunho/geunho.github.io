<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Tils on Blog · 김근호(Kim, Geunho)</title>
    <link>https://blog.geunho.dev/til/</link>
    <description>Recent content in Tils on Blog · 김근호(Kim, Geunho)</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 08 Jun 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://blog.geunho.dev/til/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Maven - 의존성 관리</title>
      <link>https://blog.geunho.dev/til/maven/dependencies/</link>
      <pubDate>Mon, 08 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://blog.geunho.dev/til/maven/dependencies/</guid>
      <description>Maven 프로젝트는 외부 모듈을 참조할때 maven repository로부터 참조 대상을 POM 파일의 의존성 항목에 추가해서 관리한다.
&amp;lt;dependencies&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;$groupId&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;$artifactId&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;$version&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; &amp;lt;/dependencies&amp;gt; 참조로 하는 모듈은 maven import시 프로젝트의 classpath에 참조 대상 라이브러리로 추가되고, 만약 그 모듈이 다시 참조하는 모듈이 있는 경우 그 모듈도 classpath에 함께 추가된다.
이로인해 발생하는 주의해야 할 부분이 발생한다.
 classpath에 참조의 하위 참조도 추가되므로 root 프로젝트에서 해당 모듈에 접근할 수 있음 만약 root 프로젝트에서 하위 참조를 즉시 참조로 추가해서 사용할 경우 버전 충돌이 일어날 수 있음 만약 즉시 참조없이 사용할 경우 상위 모듈 참조를 제거했을 때 빌드 오류가 발생할 수 있음  따라서 어떤 모듈을 의존성에 추가할때, 하위 참조 모듈은 classpath에 추가되지 않도록 설정하거나 -(1) 상위 모듈을 제거할 때 하위 참조 모듈을 명시적으로 의존성에 추가 -(2) 해줘야 한다.</description>
    </item>
    
    <item>
      <title>GitHub - 코멘트에 접어두는 코드 삽입하기</title>
      <link>https://blog.geunho.dev/til/github/markdown-tips-1/</link>
      <pubDate>Fri, 05 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://blog.geunho.dev/til/github/markdown-tips-1/</guid>
      <description>GitHub에서 마크다운 형식의 코멘트를 남기다보면 장황한 부가 설명은 접어두는 블럭으로 지정할 수 있다.
접어두는 블럭은 details, summary 태그를 코멘트에 직접 넣어주면 되는데 다음과 같이 사용할 수 있다.
&amp;lt;details&amp;gt; &amp;lt;summary&amp;gt;더 보기...&amp;lt;/summary&amp;gt; 장황한 부연 설명들 &amp;lt;/details&amp;gt; &amp;gt; 더 보기... (클릭해서 펼치고 접어둘 수 있음) 이때 코드 뭉치를 넣을 수 도 있는데, 한 줄 넘기고 작성하면 잘 동작한다.
&amp;lt;details&amp;gt; &amp;lt;summary&amp;gt;더 보기...&amp;lt;/summary&amp;gt; [장황한 code block] &amp;lt;/details&amp;gt; </description>
    </item>
    
    <item>
      <title>Redis - 조회 명령어 정리</title>
      <link>https://blog.geunho.dev/til/redis/ops/</link>
      <pubDate>Thu, 04 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://blog.geunho.dev/til/redis/ops/</guid>
      <description>redis-cli를 이용해서 간단히 조회할 때 사용할만한 redis 명령어 몇 가지를 정리한다.
get string 형식으로 저장된 값 하나를 조회한다.
get my-key &amp;#34;my value blah blah&amp;#34; hget hash로 저장된 값의 필드 하나를 조회한다.
# my-hkey로 저장된 hash의 my-field에 대한 값을 조회 hget my-hkey my-field &amp;#34;my field value blah blah&amp;#34; lrange list로 저장된 목록을 조회한다.
# 전체 목록 조회 lrange my-lkey 0 -1 # 일부 목록 조회, 시작 인덱스와 끝 인덱스를 지정 # 끝 인덱스는 결과에 포함됨 lrange my-lkey 1 4 smembers set으로 저장된 모든 값을 조회한다.</description>
    </item>
    
    <item>
      <title>from_timestamp - Impala 날짜 형식 사용하기</title>
      <link>https://blog.geunho.dev/til/impala/from_timestamp/</link>
      <pubDate>Wed, 03 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://blog.geunho.dev/til/impala/from_timestamp/</guid>
      <description>일반적인 RDB에 내장된 to_char(timestamp, text) 함수가 Impala에는 없다. 대신, 같은 역할을 하면서 이름만 다른 from_timestamp(TIMESTAMP datetime, STRING pattern) 함수를 사용하면 된다.
# ANSI SELECT to_char(now(), &amp;#39;yyyy-MM-dd, HH:mm&amp;#39;) # Impala SELECT from_timestamp(now(), &amp;#39;yyyy-MM-dd, HH:mm&amp;#39;) 참고  Impala Date and Time Functions#FROM_TIMESTAMP, Impala docs 9.8. Data Type Formatting Functions, Postgresql docs  </description>
    </item>
    
    <item>
      <title>Kudu - 기본 내용 정리 (1)</title>
      <link>https://blog.geunho.dev/til/kudu/101/</link>
      <pubDate>Wed, 03 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://blog.geunho.dev/til/kudu/101/</guid>
      <description>101 Apache Kudu는 Cloudera에서 개발한 컬럼 기반(Columnar) 저장소이다. 저장소 자체는 다른 서비스(예. HDSF)에 대한 의존 없이, Kudu 인스턴스(Tablet)만으로 클러스터를 이루며 리더 선정도 내장된 알고리즘(Raft consensus algorithm)에 의해 정해진다.
Kudu는 물리적으로 레코드를 컬럼 단위로 저장하기 때문에, 일부 컬럼에 대한 질의시 네트워크 비용을 아끼면서 효율적으로 레코드를 조회할 수 있다. (컬럼 지향(Column oriented) 저장소인 HBase는 물리적으로 로우 단위로 저장)
Kudu에 질의하기 위해서는 별도의 질의 엔진이 필요하다. MR, Spark와 같은 Hadoop 계열 엔진을 사용할 수 있는데, 아무래도 같은 Cloudera에서 개발한 Impala를 권장한다.</description>
    </item>
    
    <item>
      <title>har - Hadoop archive 도구로 압축하기</title>
      <link>https://blog.geunho.dev/til/hadoop/har/</link>
      <pubDate>Tue, 02 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://blog.geunho.dev/til/hadoop/har/</guid>
      <description>hadoop 명령어에는 HDFS 상 파일들을 압축하기 위한 archive 도구가 있다. 명령어 한 줄로 여러 경로의 입력들을 하나로 압축할 수 있다.
HDFS의 블록 크기보다 작은 다수 개의 파일은 namenode의 메모리 공간을 낭비하는데, 이러한 파일들을 har 압축하면 사용량을 줄일 수 있다.
단, 압축시 원본에 대해 복사본을 생성하는 것이므로 원본 파일 용량만큼의 공간은 확보되어 있어야 한다. 그리고 하나의 파일로 압축하더라도, MR 입력시 har 단일 파일로 인식하지 않고 원본 파일 개수 만큼을 입력으로 가지므로 MR의 작업 효율이 좋아지지는 않는다.</description>
    </item>
    
    <item>
      <title>sh - sed로 마지막 N개 문자열 제거하기</title>
      <link>https://blog.geunho.dev/til/sh/sed-tips-1/</link>
      <pubDate>Sun, 31 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://blog.geunho.dev/til/sh/sed-tips-1/</guid>
      <description>쉘에서 sed (stream editor) 도구를 사용하면 다채로운 문자열 조작을 할 수 있다.
사용방법 아래는 마지막 2개 문자열을 제거하는 예제이다.
# sed s/regexp/replacement/ sed &amp;#39;s/.\{2\}$//&amp;#39; &amp;lt;&amp;lt;&amp;lt; &amp;#34;Hello, Bash&amp;#34; Hello, Ba # 위 예시는 마지막 2개 문자를 공백으로 치환한 것. 따라서 다른 문자열로 치환도 가능하다. sed &amp;#39;s/.\{2\}$/bo/&amp;#39; &amp;lt;&amp;lt;&amp;lt; &amp;#34;Hello, Bash&amp;#34; Hello, Babo 참고 https://linux.die.net/man/1/sed</description>
    </item>
    
    <item>
      <title>distcp - HDFS 클러스터 간 파일 복제하기</title>
      <link>https://blog.geunho.dev/til/hadoop/distcp/</link>
      <pubDate>Fri, 29 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://blog.geunho.dev/til/hadoop/distcp/</guid>
      <description>distcp는 distributed copy의 약어로, 서로다른 HDFS 클러스터 간 파일을 복제할때 사용하는 명령어이다.
사용방법 hadoop distcp hdfs://nn1/job/logs hdfs://nn1/job/others hdfs://nn2/job/ nn2 hdfs에 nn1의 두 디렉토리를 복제한다. 공백으로 구분해서 입력 경로를 여러 개를 지정할 수 있고, 마지막 인자에 출력 경로를 지정하면 된다.
hadoop distcp -D dfs.replication=2 -m 32 hdfs://nn1/job/logs hdfs://nn1/job/others hdfs://nn2/job/ -D 옵션으로 hadoop 프로그램에 일반적인 옵션을 지정할 수 있고, -m 옵셥으로 map 수를 지정하여 최대 동시 복제 수를 설정한다.
참고 DistCp, Hadoop docs</description>
    </item>
    
  </channel>
</rss>